each LLM can take a particular number of tokens at a time:
we are using Llama: 4096 tokens at a time
thats why we pass the input info in the form of chunks (like a paragraph in a page)
